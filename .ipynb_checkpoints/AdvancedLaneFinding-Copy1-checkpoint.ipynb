{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Lane Finding Project\n",
    "\n",
    "Steps:\n",
    "\n",
    "- Compute camera calibration matrix and distortion coefficients using chessboard images\n",
    "- Apply a distortion correction to raw images.\n",
    "- Thresholding\n",
    "- Perspective Transformation\n",
    "- Finding Lane Lines\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_folder = 'output_images'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the camera calibration using chessboard images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calibrate_camera(cal_images, nx, ny):\n",
    "    objpoints = []  # 3D points\n",
    "    imgpoints = []  # 2D points\n",
    "\n",
    "    objp = np.zeros((nx*ny,3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:nx,0:ny].T.reshape(-1, 2)\n",
    "\n",
    "    fname = cal_images[0]\n",
    "    for fname in cal_images:\n",
    "        img = cv2.imread(fname)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "        if ret == True:\n",
    "            objpoints.append(objp)\n",
    "            imgpoints.append(corners)\n",
    "            \n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1],None,None)\n",
    "    \n",
    "    return mtx, dist\n",
    "\n",
    "def camera_setup():\n",
    "    cal_images = glob.glob('camera_cal/calibration*.jpg')\n",
    "    nx, ny = 9, 6\n",
    "    cam_mtx, cam_dist = calibrate_camera(cal_images, nx, ny)\n",
    "    return cam_mtx, cam_dist\n",
    "\n",
    "cam_mtx, cam_dist = camera_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline (test images)\n",
    "## 1) Apply a distortion correction to sample test street images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function for plotting an images before and after imageprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_images(original, processed, title):\n",
    "    # Plot the result\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(original)\n",
    "    ax1.set_title('Original Image', fontsize=50)\n",
    "    ax2.imshow(processed, cmap='gray')\n",
    "    ax2.set_title(title, fontsize=50)\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Describe how (and identify where in your code) you used color transforms, gradients or other methods to create a thresholded binary image. Provide an example of a binary image result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following the example codes of the lecture\n",
    "## Get Binary Image with Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def abs_sobel_thresh(img, orient='x', sobel_kernel=3, thresh=(0,255)):\n",
    "    # 1) Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # 2) Take the derivative in x or y given orient = 'x' or 'y'\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    # 3) Take the absolute value of the derivative or gradient\n",
    "    abs_sobelx = np.absolute(sobelx)\n",
    "    # 4) Scale to 8-bit (0 - 255) then convert to type = np.uint8\n",
    "    scaled_sobel = np.uint8(255*abs_sobelx/np.max(abs_sobelx))\n",
    "    # 5) Create a mask of 1's where the scaled gradient magnitude \n",
    "            # is > thresh_min and < thresh_max\n",
    "    sxbinary = np.zeros_like(scaled_sobel)\n",
    "    sxbinary[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "    # 6) Return this mask as your binary_output image\n",
    "    return sxbinary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magnitude of the Gradient\n",
    "Next we are using the magnitude of the gradient using the Sobel operator in both the x and y direction. \n",
    "\n",
    "However, there are still some thick lines in the middle that could confuse the lane detector. We also notice that both gradients fail to detect the yellow lane at all. We'll need to fix this if we want to be able to drive anywhere with yellow lanes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function that applies Sobel x and y, \n",
    "# then computes the magnitude of the gradient\n",
    "# and applies a threshold\n",
    "def mag_thresh(img, sobel_kernel=3, mag_thresh=(0, 255)):\n",
    "    \n",
    "    # Apply the following steps to img\n",
    "    # 1) Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # 2) Take the gradient in x and y separately\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    # 3) Calculate the magnitude \n",
    "    magnitude = np.sqrt(sobelx*sobelx + sobely*sobely)\n",
    "    # 5) Scale to 8-bit (0 - 255) and convert to type = np.uint8\n",
    "    scaled_sobel = np.uint8(255*magnitude/np.max(magnitude))\n",
    "    # 6) Create a binary mask where mag thresholds are met\n",
    "    sxbinary = np.zeros_like(scaled_sobel)\n",
    "    sxbinary[(scaled_sobel >= mag_thresh[0]) & (scaled_sobel <= mag_thresh[1])] = 1\n",
    "    # 7) Return this mask as your binary_output image\n",
    "    return sxbinary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direction of the Gradient\n",
    "In order to detect those yellow lanes, we can compute the direction of the gradient as the arctangent of the gradient in the y direction divided by the gradient in the x direction. This is a much noisier gradient than our magnitude gradient, but it accurately captures the identical direction of the pixels from the yellow lane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function that applies Sobel x and y, \n",
    "# then computes the direction of the gradient\n",
    "# and applies a threshold.\n",
    "def dir_threshold(img, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    \n",
    "    # Apply the following steps to img\n",
    "    # 1) Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # 2) Take the gradient in x and y separately\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    # 3) Take the absolute value of the x and y gradients\n",
    "    abs_sobelx = np.abs(sobelx)\n",
    "    abs_sobely = np.abs(sobely)\n",
    "    # 4) Use np.arctan2(abs_sobely, abs_sobelx) to calculate the direction of the gradient \n",
    "    direction = np.arctan2(abs_sobely, abs_sobelx)\n",
    "    # 5) Create a binary mask where direction thresholds are met\n",
    "    sbinary = np.zeros_like(direction)\n",
    "    sbinary[(direction >= thresh[0]) & (direction <= thresh[1])] = 1\n",
    "    # 6) Return this mask as your binary_output image\n",
    "    return sbinary\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Thresholds\n",
    "\n",
    "Now that we've isolated the white lanes with magnitude thresholding and the yellow lanes with direction thresholding, we can combine those images with base x/y sobel thresholds to get a result that captures both lanes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combined different thresholding techniques\n",
    "def combined_thresh(img):\n",
    "    # Choose a Sobel kernel size\n",
    "    ksize = 11\n",
    "\n",
    "    # Apply each of the thresholding functions\n",
    "    gradx = abs_sobel_thresh(img, orient='x', sobel_kernel=ksize, thresh=(20,100))\n",
    "    grady = abs_sobel_thresh(img, orient='y', sobel_kernel=ksize, thresh=(20,100))\n",
    "    mag_binary = mag_thresh(img, sobel_kernel=7, mag_thresh=(50, 100))\n",
    "    dir_binary = dir_threshold(img, sobel_kernel=15, thresh=(0.4, 1.3))\n",
    "    \n",
    "    #Combine them\n",
    "    combined = np.zeros_like(dir_binary)\n",
    "    combined[((gradx == 1) | (grady == 1)) & ((mag_binary == 1) | (dir_binary == 1))] = 1\n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Thresholding\n",
    "\n",
    "In this step, we isolat the lightness and saturation channels of the color image and then take an absolute Sobel in the x direction of the lightness channel. Finally we compute a binary that activates a pixel if it's saturated pixel is within the hardcoded threshold OR if it's scaled Sobel pixel of the lightness channel is within a separate hardcoded threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Edit this function to create your own pipeline.\n",
    "def pipeline(img, s_thresh=(170, 255), sx_thresh=(50, 100)):\n",
    "    img = np.copy(img)\n",
    "    # Convert to HLS color space and separate the S channel\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HLS).astype(np.float)\n",
    "    l_channel = hsv[:,:,1]\n",
    "    s_channel = hsv[:,:,2]\n",
    "    \n",
    "    # Sobel x\n",
    "    sobelx = cv2.Sobel(l_channel, cv2.CV_64F, 1, 0) # Take the derivative in x\n",
    "    abs_sobelx = np.absolute(sobelx) # Absolute x derivative to accentuate lines away from horizontal\n",
    "    scaled_sobel = np.uint8(255*abs_sobelx/np.max(abs_sobelx))\n",
    "    \n",
    "    # Threshold x gradient and color\n",
    "    color_gradient_binary = np.zeros_like(s_channel)\n",
    "    color_gradient_binary[((s_channel >= s_thresh[0]) & (s_channel <= s_thresh[1])) | ((scaled_sobel >= sx_thresh[0]) & (scaled_sobel <= sx_thresh[1]))] = 1\n",
    "    return color_gradient_binary\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Perspective Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Box for Perspective Transform\n",
    "Next, you want to identify four source points for your perspective transform. In this case, you can assume the road is a flat plane. This isn't strictly true, but it can serve as an approximation for this project. You would like to pick four points in a trapezoidal shape (similar to region masking) that would represent a rectangle when looking down on the road from above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def perspective_transform(img, mtx, dist, isColor=True):\n",
    "\n",
    "    xoffset = 50 # offset for dst points\n",
    "    yoffset = 0\n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "\n",
    "   \n",
    "    src = np.float32([(200, 700),(600, 450), (700,450),(1080, 700)])\n",
    "  \n",
    "   \n",
    "    dst=np.float32([[350, 700], [350, 0],      [950, 0],[950, 700]]) \n",
    "    # Given src and dst points, calculate the perspective transform matrix\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    # Warp the image using OpenCV warpPerspective()\n",
    "    warped = cv2.warpPerspective(img, M, img_size)\n",
    "\n",
    "    # Return the resulting image and matrix\n",
    "    return warped, M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region of Interest (ROI) Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def region_of_interest(img):\n",
    "    \"\"\"Extracts region of interest from an image.\n",
    "    \n",
    "    Args:\n",
    "      img: Image from which to extract region of interest.\n",
    "      roi_vertices: Numpy array of x,y points specifying region of interest.\n",
    "    Returns:\n",
    "      New image containing only the pixels within the region of interest.\n",
    "    \"\"\"\n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    roi_vertices = np.array([[(150,height),((int(width/2)-20), int(height*3/5)), \n",
    "        (int((width/2)+50), int(height*3/5)), (width-50,height)]], dtype=np.int32)\n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "    mask = np.zeros_like(img)\n",
    "    cv2.fillPoly(mask, roi_vertices, 255)\n",
    "    masked_img = cv2.bitwise_and(img, mask)\n",
    "    return masked_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class For Tracking Right and Left Lane Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Line():\n",
    "    def __init__(self,n=5):\n",
    "        self.n = n\n",
    "        #number of fits in buffer\n",
    "        self.n_buffered = 0\n",
    "        # was the line detected in the last iteration?\n",
    "        self.detected = False  \n",
    "        # x values of the last n fits of the line\n",
    "       \n",
    "        self.avgx = None\n",
    "        # fit coeffs of the last n fits\n",
    "        self.recent_fit_coeffs = deque([],maxlen=n)        \n",
    "        #polynomial coefficients averaged over the last n iterations\n",
    "        self.avg_fit_coeffs = None  \n",
    "        # xvals of the most recent fit\n",
    "        self.current_fit_xvals = [np.array([False])]  \n",
    "        #polynomial coefficients for the most recent fit\n",
    "        self.current_fit_coeffs = [np.array([False])]          \n",
    "        #x values for detected line pixels\n",
    "        \n",
    "        #radius of curvature of the line in some units\n",
    "        self.radius_of_curvature = None \n",
    "        # origin (pixels) of fitted line at the bottom of the image\n",
    "        self.line_pos = None \n",
    "        #distance in meters of vehicle center from the line\n",
    "        self.line_base_pos = None \n",
    "        #difference in fit coefficients between last and new fits\n",
    "        self.diffs = np.array([0,0,0], dtype='float') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dectect_line_sliding_window(binary_warped, left=True):\n",
    "    #code from Udacity lesson\n",
    "    # Assuming you have created a warped binary image called \"binary_warped\"\n",
    "    # Take a histogram of the bottom half of the image\n",
    "    histogram = np.sum(binary_warped[np.int(binary_warped.shape[0]/2):,:], axis=0)\n",
    "      # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]/2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "    \n",
    "    if left:\n",
    "        x_base=np.argmax(histogram[:midpoint])\n",
    "    else:    \n",
    "        x_base=np.argmax(histogram[midpoint:]) + midpoint\n",
    "    # Choose the number of sliding windows\n",
    "    nwindows = 9\n",
    "    # Set height of windows\n",
    "    window_height = np.int(binary_warped.shape[0]/nwindows)\n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    x_current=x_base\n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 100\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 50\n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    lane_inds=[]\n",
    "    # Step through the windows one by one\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = binary_warped.shape[0] - (window+1)*window_height\n",
    "        win_y_high = binary_warped.shape[0] - window*window_height\n",
    "        \n",
    "        win_x_low=x_current-margin\n",
    "        win_x_high=x_current+margin\n",
    "       \n",
    "        # Identify the nonzero pixels in x and y within the window\n",
    "        good_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_x_low) & (nonzerox < win_x_high)).nonzero()[0]\n",
    "        \n",
    "        # Append these indices to the lists\n",
    "        lane_inds.append(good_inds)\n",
    "        # If you found > minpix pixels, recenter next window on their mean position\n",
    "        if len(good_inds) > minpix:\n",
    "            x_current = np.int(np.mean(nonzerox[good_inds]))\n",
    "   \n",
    "    # Concatenate the arrays of indices\n",
    "    lane_inds = np.concatenate(lane_inds)  \n",
    " \n",
    "    # Extract pixel positions\n",
    "    x = nonzerox[lane_inds]\n",
    "    y = nonzeroy[lane_inds] \n",
    " \n",
    "    # Fit a second order polynomial to each\n",
    "    fit = np.polyfit(y, x, 2)\n",
    "    \n",
    "    print(fit)\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dectect_line_fitted(binary_warped,fit):\n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    margin = 100\n",
    "    lane_inds = ((nonzerox > (fit[0]*(nonzeroy**2) + fit[1]*nonzeroy + fit[2] - margin)) & \\\n",
    "                 (nonzerox < (fit[0]*(nonzeroy**2) + fit[1]*nonzeroy + fit[2] + margin))) \n",
    "    x = nonzerox[lane_inds]\n",
    "    y = nonzeroy[lane_inds] \n",
    "     # Fit a second order polynomial to each\n",
    "    fit = np.polyfit(y, x, 2)\n",
    "    \n",
    "    print(fit)\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -9.32007094e-05   1.88887122e-01   8.50746063e+02]\n",
      "[ -9.32007094e-05   1.88887122e-01   8.50746063e+02]\n",
      "----------\n",
      "[ -4.19841875e-05   1.18970802e-01   8.81383414e+02]\n",
      "[ -4.19841875e-05   1.18970802e-01   8.81383414e+02]\n",
      "----------\n",
      "[  2.98746133e-04  -3.22655886e-01   1.06132595e+03]\n",
      "[  2.98746133e-04  -3.22655886e-01   1.06132595e+03]\n",
      "----------\n",
      "[ -4.29743589e-04   6.77491097e-01   7.28377856e+02]\n",
      "[ -4.32764788e-04   6.80223796e-01   7.27800949e+02]\n",
      "----------\n",
      "[  2.84275725e-04  -3.52897515e-01   1.07703498e+03]\n",
      "[  2.84275725e-04  -3.52897515e-01   1.07703498e+03]\n",
      "----------\n",
      "[  2.93544748e-04  -2.89174081e-01   1.05614931e+03]\n",
      "[  2.93544748e-04  -2.89174081e-01   1.05614931e+03]\n",
      "----------\n",
      "[  8.16055473e-04  -7.14907855e-01   1.09869584e+03]\n",
      "[  8.17193002e-04  -7.15703278e-01   1.09875541e+03]\n",
      "----------\n",
      "[  2.06149681e-04  -2.71644762e-01   1.08245691e+03]\n",
      "[  2.06149681e-04  -2.71644762e-01   1.08245691e+03]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def detect_lanes(img):\n",
    "    undist = cv2.undistort(img,cam_mtx, cam_dist, None, cam_mtx)\n",
    "    combined = pipeline(undist)*255.0\n",
    "    roi_img=region_of_interest(combined)\n",
    "    binary_warped, M = perspective_transform(roi_img, cam_mtx, cam_dist)\n",
    "  #  left_fit=dectect_line_sliding_window(binary_warped,left=True)\n",
    "    right_fit=dectect_line_sliding_window(binary_warped,left=False)\n",
    "    dectect_line_fitted(binary_warped,right_fit)\n",
    "    print(\"----------\")\n",
    "    \n",
    "    \n",
    "images = ['straight_lines1.jpg','straight_lines2.jpg','test1.jpg','test2.jpg','test3.jpg','test4.jpg','test5.jpg','test6.jpg']\n",
    "\n",
    "for i in range(len(images)):\n",
    "    img = cv2.imread('test_images/' + images[i])  \n",
    "    img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    detect_lanes(img)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
